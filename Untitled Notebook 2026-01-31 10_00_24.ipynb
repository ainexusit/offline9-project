{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28aaf523-e5c9-4d2e-bcc9-aa9a52384ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark =  SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cca6bb3-580a-4a4d-8e0c-3f11a4bc9cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empdata = [(101,'Keshav','Pune'),(502,'Ravi','Mumbai'),(103,'Rahul','Chennai'),(700,'Madhav','Mumbai'),(105,'Ravi','Mumbai'),(106,'Ravi','Mumbai'),(107,'Ravi','Mumbai'),(108,'Ravi','Mumbai')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f88c6b-7a45-4f9b-a1e7-1c381b59ce53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "department = ([(101,'IT'),(102,'HR'),(103,'Admin'),(104,'IT'),(105,'HR'),(106,'Operatoins'),(107,'Sales'),(108,'Marketing'),(500,'Finance')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8368e7-1292-458d-8a97-17333c548b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name|   city|\n+---+------+-------+\n|101|Keshav|   Pune|\n|502|  Ravi| Mumbai|\n|103| Rahul|Chennai|\n|700|Madhav| Mumbai|\n|105|  Ravi| Mumbai|\n|106|  Ravi| Mumbai|\n|107|  Ravi| Mumbai|\n|108|  Ravi| Mumbai|\n+---+------+-------+\n\n+---+----------+\n| id|department|\n+---+----------+\n|101|        IT|\n|102|        HR|\n|103|     Admin|\n|104|        IT|\n|105|        HR|\n|106|Operatoins|\n|107|     Sales|\n|108| Marketing|\n|500|   Finance|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(empdata,['id','name','city'])\n",
    "df2 = spark.createDataFrame(department,['id','department'])\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa81f092-65db-4c3d-9e25-d69c3086393a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonProject [id#13410L, name#13408, city#13409, department#13411]\n         +- PhotonBroadcastHashJoin [id#13407L], [id#13410L], RightOuter, BuildLeft, false, true\n            :- PhotonShuffleExchangeSource\n            :  +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#8737]\n            :     +- PhotonShuffleExchangeSink SinglePartition\n            :        +- PhotonRowToColumnar\n            :           +- LocalTableScan [id#13407L, name#13408, city#13409]\n            +- PhotonRowToColumnar\n               +- LocalTableScan [id#13410L, department#13411]\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,on=\"id\",how=\"right\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffdcb3b5-4ded-4a36-941c-b5067b7a3ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonProject [id#13485L, name#13486, city#13487, department#13489]\n         +- PhotonBroadcastHashJoin [id#13485L], [id#13488L], Inner, BuildRight, false, true\n            :- PhotonRowToColumnar\n            :  +- LocalTableScan [id#13485L, name#13486, city#13487]\n            +- PhotonShuffleExchangeSource\n               +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#9128]\n                  +- PhotonShuffleExchangeSink SinglePartition\n                     +- PhotonRowToColumnar\n                        +- LocalTableScan [id#13488L, department#13489]\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "df1.join(broadcast(df2), \"id\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521e28bc-b859-4388-9fc7-2cfde743c551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+----------+\n| id|  name|   city|department|\n+---+------+-------+----------+\n|101|Keshav|   Pune|        IT|\n|103| Rahul|Chennai|     Admin|\n|105|  Ravi| Mumbai|        HR|\n|106|  Ravi| Mumbai|Operatoins|\n|107|  Ravi| Mumbai|     Sales|\n|108|  Ravi| Mumbai| Marketing|\n+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2.hint(\"merge\"), \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "895ada78-2b8a-4e0b-9d0c-1f312ef0a1f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+----------+\n| id|  name|   city|department|\n+---+------+-------+----------+\n|101|Keshav|   Pune|        IT|\n|502|  Ravi| Mumbai|      NULL|\n|103| Rahul|Chennai|     Admin|\n|700|Madhav| Mumbai|      NULL|\n|105|  Ravi| Mumbai|        HR|\n|106|  Ravi| Mumbai|Operatoins|\n|107|  Ravi| Mumbai|     Sales|\n|108|  Ravi| Mumbai| Marketing|\n+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2.hint(\"shuffle_hash\"), \"id\",how='left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375de87b-db37-432c-bea0-93cb4579bd1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+---+----------+\n| id|  name|   city| id|department|\n+---+------+-------+---+----------+\n|101|Keshav|   Pune|101|        IT|\n|101|Keshav|   Pune|102|        HR|\n|101|Keshav|   Pune|103|     Admin|\n|101|Keshav|   Pune|104|        IT|\n|101|Keshav|   Pune|105|        HR|\n|101|Keshav|   Pune|106|Operatoins|\n|101|Keshav|   Pune|107|     Sales|\n|101|Keshav|   Pune|108| Marketing|\n|101|Keshav|   Pune|500|   Finance|\n|502|  Ravi| Mumbai|101|        IT|\n|502|  Ravi| Mumbai|102|        HR|\n|502|  Ravi| Mumbai|103|     Admin|\n|502|  Ravi| Mumbai|104|        IT|\n|502|  Ravi| Mumbai|105|        HR|\n|502|  Ravi| Mumbai|106|Operatoins|\n|502|  Ravi| Mumbai|107|     Sales|\n|502|  Ravi| Mumbai|108| Marketing|\n|502|  Ravi| Mumbai|500|   Finance|\n|103| Rahul|Chennai|101|        IT|\n|103| Rahul|Chennai|102|        HR|\n+---+------+-------+---+----------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2.hint(\"cartesion\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1712e825-3aba-4797-b048-391458ebf7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+---+----------+\n| id|  name|   city| id|department|\n+---+------+-------+---+----------+\n|502|  Ravi| Mumbai|101|        IT|\n|502|  Ravi| Mumbai|102|        HR|\n|502|  Ravi| Mumbai|103|     Admin|\n|502|  Ravi| Mumbai|104|        IT|\n|502|  Ravi| Mumbai|105|        HR|\n|502|  Ravi| Mumbai|106|Operatoins|\n|502|  Ravi| Mumbai|107|     Sales|\n|502|  Ravi| Mumbai|108| Marketing|\n|502|  Ravi| Mumbai|500|   Finance|\n|103| Rahul|Chennai|101|        IT|\n|103| Rahul|Chennai|102|        HR|\n|700|Madhav| Mumbai|101|        IT|\n|700|Madhav| Mumbai|102|        HR|\n|700|Madhav| Mumbai|103|     Admin|\n|700|Madhav| Mumbai|104|        IT|\n|700|Madhav| Mumbai|105|        HR|\n|700|Madhav| Mumbai|106|Operatoins|\n|700|Madhav| Mumbai|107|     Sales|\n|700|Madhav| Mumbai|108| Marketing|\n|700|Madhav| Mumbai|500|   Finance|\n+---+------+-------+---+----------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2,df1.id>df2.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdbda2d4-04aa-4816-94e5-953dc399c4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name|   city|\n+---+------+-------+\n|101|Keshav|   Pune|\n|502|  Ravi| Mumbai|\n|103| Rahul|Chennai|\n|700|Madhav| Mumbai|\n|105|  Ravi| Mumbai|\n|106|  Ravi| Mumbai|\n|107|  Ravi| Mumbai|\n|108|  Ravi| Mumbai|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e9cf5f6-3d1a-4260-baf7-a80f77937ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n| id|department|\n+---+----------+\n|101|        IT|\n|102|        HR|\n|103|     Admin|\n|104|        IT|\n|105|        HR|\n|106|Operatoins|\n|107|     Sales|\n|108| Marketing|\n|500|   Finance|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6e4f70-3e9c-4998-b8fe-5ba6e67703a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+----------+\n| id|  name|   city|id_squared|\n+---+------+-------+----------+\n|101|Keshav|   Pune|     10201|\n|502|  Ravi| Mumbai|    252004|\n|103| Rahul|Chennai|     10609|\n|700|Madhav| Mumbai|    490000|\n|105|  Ravi| Mumbai|     11025|\n|106|  Ravi| Mumbai|     11236|\n|107|  Ravi| Mumbai|     11449|\n|108|  Ravi| Mumbai|     11664|\n+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def id_double(id):\n",
    "    return id**2\n",
    "\n",
    "udf_id_double = udf(id_double)\n",
    "df1.withColumn(\"id_squared\",udf_id_double(df1.id)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c2869e-23cc-417f-895a-db52db748de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name|   city|\n+---+------+-------+\n|101|Keshav|   Pune|\n|502|  Ravi| Mumbai|\n|103| Rahul|Chennai|\n|700|Madhav| Mumbai|\n|105|  Ravi| Mumbai|\n|106|  Ravi| Mumbai|\n|107|  Ravi| Mumbai|\n|108|  Ravi| Mumbai|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5cbd25-17cf-4d79-9f9a-7ce62e168a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/workspace/default/offline9/deptwise_Sal.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14bd70ed-e3b5-456a-9a8a-4ac8b83abe92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----+---------+\n|empid|         name|  sal|     dept|\n+-----+-------------+-----+---------+\n|  101|     John Doe|55000|       IT|\n|  102|   Jane Smith|62000|       HR|\n|  103|Michael Brown|58000|  Finance|\n|  104|  Sara Wilson|60000|Marketing|\n|  105|David Johnson|53000|       IT|\n|  106|  Emily Davis|61000|       HR|\n|  107| Chris Martin|57000|  Finance|\n|  108|   Amanda Lee|59000|Marketing|\n|  109|  Robert King|54000|       IT|\n|  110|  Laura Scott|63000|       HR|\n|  111|Daniel Turner|56000|  Finance|\n|  112|Olivia Harris|60500|Marketing|\n|  113| Ethan Walker|57500|       IT|\n|  114|  Grace Young|61500|       HR|\n|  115| Brandon Hill|58500|  Finance|\n|  116|    Mia Adams|59500|Marketing|\n|  117|  Kevin Baker|54500|       IT|\n|  118|    Zoe Clark|62500|       HR|\n|  119| Jason Wright|56500|  Finance|\n|  120|   Nora Green|60000|Marketing|\n+-----+-------------+-----+---------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62955fd7-9bca-46cf-b4a6-fa06ad23f9d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[empid: int, name: string, sal: int, dept: string, first_name: string]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_first_name(name):\n",
    "    return name.split(\" \")[0]\n",
    "get_first_name_udf = udf(get_first_name)\n",
    "df.withColumn(\"first_name\",get_first_name_udf(df.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f376fcf-b743-483a-aa15-68d9045cc818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----+---------+----------+\n|empid|         name|  sal|     dept|first_name|\n+-----+-------------+-----+---------+----------+\n|  101|     John Doe|55000|       IT|      John|\n|  102|   Jane Smith|62000|       HR|      Jane|\n|  103|Michael Brown|58000|  Finance|   Michael|\n|  104|  Sara Wilson|60000|Marketing|      Sara|\n|  105|David Johnson|53000|       IT|     David|\n|  106|  Emily Davis|61000|       HR|     Emily|\n|  107| Chris Martin|57000|  Finance|     Chris|\n|  108|   Amanda Lee|59000|Marketing|    Amanda|\n|  109|  Robert King|54000|       IT|    Robert|\n|  110|  Laura Scott|63000|       HR|     Laura|\n|  111|Daniel Turner|56000|  Finance|    Daniel|\n|  112|Olivia Harris|60500|Marketing|    Olivia|\n|  113| Ethan Walker|57500|       IT|     Ethan|\n|  114|  Grace Young|61500|       HR|     Grace|\n|  115| Brandon Hill|58500|  Finance|   Brandon|\n|  116|    Mia Adams|59500|Marketing|       Mia|\n|  117|  Kevin Baker|54500|       IT|     Kevin|\n|  118|    Zoe Clark|62500|       HR|       Zoe|\n|  119| Jason Wright|56500|  Finance|     Jason|\n|  120|   Nora Green|60000|Marketing|      Nora|\n+-----+-------------+-----+---------+----------+\nonly showing top 20 rows\n+-----+-------------+-----+---------+\n|empid|         name|  sal|     dept|\n+-----+-------------+-----+---------+\n|  101|     John Doe|55000|       IT|\n|  102|   Jane Smith|62000|       HR|\n|  103|Michael Brown|58000|  Finance|\n|  104|  Sara Wilson|60000|Marketing|\n|  105|David Johnson|53000|       IT|\n|  106|  Emily Davis|61000|       HR|\n|  107| Chris Martin|57000|  Finance|\n|  108|   Amanda Lee|59000|Marketing|\n|  109|  Robert King|54000|       IT|\n|  110|  Laura Scott|63000|       HR|\n|  111|Daniel Turner|56000|  Finance|\n|  112|Olivia Harris|60500|Marketing|\n|  113| Ethan Walker|57500|       IT|\n|  114|  Grace Young|61500|       HR|\n|  115| Brandon Hill|58500|  Finance|\n|  116|    Mia Adams|59500|Marketing|\n|  117|  Kevin Baker|54500|       IT|\n|  118|    Zoe Clark|62500|       HR|\n|  119| Jason Wright|56500|  Finance|\n|  120|   Nora Green|60000|Marketing|\n+-----+-------------+-----+---------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "@udf(returnType=StringType())\n",
    "def get_first_name(name):\n",
    "    return name.split(\" \")[0]\n",
    "\n",
    "df.withColumn(\"first_name\",get_first_name(df.name)).show()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1526df6d-1962-49c6-b088-f44353428e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-----+---------+\n|empid|         name|  sal|     dept|\n+-----+-------------+-----+---------+\n|  101|     John Doe|55000|       IT|\n|  102|   Jane Smith|62000|       HR|\n|  103|Michael Brown|58000|  Finance|\n|  104|  Sara Wilson|60000|Marketing|\n|  105|David Johnson|53000|       IT|\n|  106|  Emily Davis|61000|       HR|\n|  107| Chris Martin|57000|  Finance|\n|  108|   Amanda Lee|59000|Marketing|\n|  109|  Robert King|54000|       IT|\n|  110|  Laura Scott|63000|       HR|\n|  111|Daniel Turner|56000|  Finance|\n|  112|Olivia Harris|60500|Marketing|\n|  113| Ethan Walker|57500|       IT|\n|  114|  Grace Young|61500|       HR|\n|  115| Brandon Hill|58500|  Finance|\n|  116|    Mia Adams|59500|Marketing|\n|  117|  Kevin Baker|54500|       IT|\n|  118|    Zoe Clark|62500|       HR|\n|  119| Jason Wright|56500|  Finance|\n|  120|   Nora Green|60000|Marketing|\n+-----+-------------+-----+---------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a031b873-17c3-4dfe-b45b-1fa15142dbc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPySparkNotImplementedError\u001B[0m                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5882552701624876>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mgetNumPartitions()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2330\u001B[0m, in \u001B[0;36mDataFrame.rdd\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   2328\u001B[0m \u001B[38;5;129m@property\u001B[39m\n",
       "\u001B[1;32m   2329\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrdd\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Row]\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m-> 2330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n",
       "\u001B[1;32m   2331\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   2332\u001B[0m         messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n",
       "\u001B[1;32m   2333\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mPySparkNotImplementedError\u001B[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PySparkNotImplementedError",
        "evalue": "[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "metadata": {
        "errorSummary": "[NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOT_IMPLEMENTED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPySparkNotImplementedError\u001B[0m                Traceback (most recent call last)",
        "File \u001B[0;32m<command-5882552701624876>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mgetNumPartitions()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2330\u001B[0m, in \u001B[0;36mDataFrame.rdd\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2328\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m   2329\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrdd\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Row]\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 2330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkNotImplementedError(\n\u001B[1;32m   2331\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_IMPLEMENTED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2332\u001B[0m         messageParameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrdd\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m   2333\u001B[0m     )\n",
        "\u001B[0;31mPySparkNotImplementedError\u001B[0m: [NOT_IMPLEMENTED] Using custom code using PySpark RDDs is not allowed on serverless compute. We suggest using mapInPandas or mapInArrow for the most common use cases. For more details on compatibility and limitations, check: https://docs.databricks.com/release-notes/serverless.html#limitations"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2626cdf-807e-47b4-a059-ccbffc490138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2026-01-31 10:00:24",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}